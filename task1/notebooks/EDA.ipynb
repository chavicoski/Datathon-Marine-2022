{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e1210d8-9d7a-4cea-9d71-97e2f5f9f58c",
   "metadata": {},
   "source": [
    "# Marine Datathon 2022: Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013dd1cb-3177-4511-8591-e2e40bb118d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e8f382-9e23-4684-9a89-c243a76a2735",
   "metadata": {},
   "source": [
    "## Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcb8059-e42a-42b0-9ea2-93a264c64212",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_path = \"../data/labels.csv\"\n",
    "audio_dir = \"../data/audios\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee70459",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b585ca1-0874-4970-8a6f-a2a3bddba9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(annotations_path)\n",
    "annotations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e86b88",
   "metadata": {},
   "source": [
    "## Labels Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ea7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccadc02",
   "metadata": {},
   "source": [
    "Show the count of events durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514e030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 10))\n",
    "plt.xlabel(\"Duration (seconds)\")\n",
    "plt.ylabel(\"Count\")\n",
    "annotations.duration.plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[[\"start\", \"duration\", \"end\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02829a37",
   "metadata": {},
   "source": [
    "Check the total duration of audio data by label (**in minutes**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025bd4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.groupby(\"label\").duration.sum() / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02829a37",
   "metadata": {},
   "source": [
    "Check the mean duration of each audio event by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4410edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.groupby(\"label\").duration.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89259401",
   "metadata": {},
   "source": [
    "### Test labels extraction for Sound Event Detection (SED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a02682",
   "metadata": {},
   "source": [
    "Prepare a function to convert all the annotations from one audio into a 2D matix of (# labels, frames) to encode the labels at each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad513a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_labels = annotations.label.value_counts().index\n",
    "n_labels = len(sorted_labels)\n",
    "labels2idx = {l: i for i, l in enumerate(sorted_labels)}\n",
    "\n",
    "\n",
    "def labels_to_mask(\n",
    "    audio_annot: pd.DataFrame,\n",
    "    frame_size: int,\n",
    "    hop_size: int,\n",
    ") -> np.ndarray:\n",
    "    # Load the audio\n",
    "    signal, sample_rate = torchaudio.load(os.path.join(audio_dir, f\"{audio_annot.name}.wav\"))\n",
    "\n",
    "    # Prepare the mask 2D matrix (labels, frames)\n",
    "    n_frames = int((signal.shape[-1] - frame_size) / hop_size) + 1\n",
    "    mask = np.zeros((n_labels, n_frames))\n",
    "    # Compute some utility values\n",
    "    sample_time = 1 / sample_rate\n",
    "    frame_time = sample_time * frame_size\n",
    "    for idx, row in audio_annot.iterrows():\n",
    "        start_frame = int(row.start / frame_time)\n",
    "        end_frame = int(row.end / frame_time)\n",
    "        mask[labels2idx[row.label], start_frame:end_frame] += 1\n",
    "        \n",
    "    return mask.astype(np.bool8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4313d4",
   "metadata": {},
   "source": [
    "Apply the mask extraction (for each audi file) from the annotations temporal labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c36850",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 47  # Sample to select\n",
    "FRAME_SIZE = 512\n",
    "HOP_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e5497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_masks = annotations.groupby(\"path\").apply(lambda x: labels_to_mask(x, FRAME_SIZE, HOP_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b99742",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_audio_id = audio_masks.index[sample_idx]\n",
    "\n",
    "# Plot the corresponding mask\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.imshow(audio_masks.iloc[sample_idx], aspect=\"auto\", interpolation=\"none\", cmap=\"jet\")\n",
    "plt.yticks(range(n_labels), labels=sorted_labels)\n",
    "plt.xlabel(\"Frame\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Show the audio annotations to compare\n",
    "display(annotations[annotations.path == audio_masks.index[sample_idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154e768",
   "metadata": {},
   "source": [
    "## Audio visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3326e3ee-92e7-447b-a96f-e6c25467d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, sample_rate = torchaudio.load(os.path.join(audio_dir, f\"{selected_audio_id}.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveform(waveform, sample_rate):\n",
    "    waveform = waveform.numpy()\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    plt.plot(time_axis, waveform[0], linewidth=1)\n",
    "    plt.grid(True)\n",
    "    plt.suptitle(\"waveform\")\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\"):\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    plt.title(title or \"Spectrogram (db)\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(\"frame\")\n",
    "    amplitude_2_DB = torchaudio.transforms.AmplitudeToDB()\n",
    "    plt.imshow(amplitude_2_DB(specgram), origin=\"lower\", aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e962d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_waveform(signal, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_transform = torchaudio.transforms.Spectrogram(n_fft=FRAME_SIZE, hop_length=HOP_SIZE, center=False)\n",
    "spectrogram = spec_transform(signal)\n",
    "plot_spectrogram(spectrogram[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "melspec_transform = torchaudio.transforms.MelSpectrogram(sample_rate=50000, n_fft=FRAME_SIZE, hop_length=HOP_SIZE, center=False, n_mels=64)\n",
    "mel_spectrogram = melspec_transform(signal)\n",
    "plot_spectrogram(mel_spectrogram[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('audio')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d2252593e89e7d3aa9f5262b6f0663e084f31daf75977bd0753d2b19f4a4ff6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

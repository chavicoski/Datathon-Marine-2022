{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(\n",
    "    spectrogram: torch.Tensor,\n",
    "    interpolation: str = \"none\",\n",
    "    figsize: Tuple[int, int] = (20, 5)\n",
    "):\n",
    "    \"\"\"Plots a spectrogram with shape (n_freq_bins, n_frames). Converts\n",
    "    the values from amplitude to decibels for better visualization\n",
    "    \n",
    "    Args:\n",
    "        spectrogram (torch.Tensor): Spectrogram data in amplitude (n_freq_bins, n_frames)\n",
    "        interpolation (str): Matplotlib interpolation for plt.imshow()\n",
    "        figsize (Tuple[int, int]): Figsize of the matplotlib figure\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(\"Spectrogram (db)\")\n",
    "    plt.ylabel(\"Frequency bins\")\n",
    "    plt.xlabel(\"frame\")\n",
    "    amplitude_to_DB = torchaudio.transforms.AmplitudeToDB()\n",
    "    plt.imshow(\n",
    "        amplitude_to_DB(spectrogram),\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "        interpolation=interpolation)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_mask(\n",
    "    mask: torch.Tensor,\n",
    "    labels: List[str],\n",
    "    figsize: Tuple[int, int] = (20, 5)\n",
    "):\n",
    "    \"\"\"Plots a mask with shape (n_labels, n_frames)\n",
    "\n",
    "    Args:\n",
    "        mask (torch.Tensor): Mask data to plot (n_labels, n_frames)\n",
    "        labels (List[str]): List of labels to use in the y-axis of the plot\n",
    "        figsize (Tuple[int, int]): Figsize of the matplotlib figure\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(mask, aspect=\"auto\", interpolation=\"none\", cmap=\"jet\")\n",
    "    plt.yticks(range(len(labels)), labels=labels)\n",
    "    plt.xlabel(\"Frame\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def spec_mask_plot(\n",
    "    spectrogram: torch.Tensor,\n",
    "    mask: torch.Tensor,\n",
    "    mask_labels: List[str],\n",
    "    spec_interpolation: str = \"none\",\n",
    "    figsize: Tuple[int, int] = (20, 10)\n",
    "):\n",
    "    \"\"\"Plots a spectrogram with shape (n_freq_bins, n_frames) and a mask\n",
    "    with shape (n_labels, n_frames) in a vertical subplot configuration.\n",
    "    Converts the values of the spectrogram from amplitude to decibels for\n",
    "    better visualization\n",
    "\n",
    "    Args:\n",
    "        spectrogram (torch.Tensor): Spectrogram data in amplitude (n_freq_bins, n_frames)\n",
    "        mask (torch.Tensor): Mask data to plot (n_labels, n_frames)\n",
    "        mask_labels (List[str]): List of labels to use in the y-axis of the mask\n",
    "        spec_interpolation (str): Matplotlib interpolation for the spectrogram\n",
    "        figsize (Tuple[int, int]): Figsize of the matplotlib figure\n",
    "    \"\"\"\n",
    "    # Prepare the plots figure\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, figsize=figsize)\n",
    "    fig.subplots_adjust(hspace=0)\n",
    "\n",
    "    # Plot the spectrogram in the top plot\n",
    "    amplitude_to_DB = torchaudio.transforms.AmplitudeToDB()\n",
    "    spec_im = axs[0].imshow(\n",
    "        amplitude_to_DB(spectrogram),\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "        interpolation=spec_interpolation)\n",
    "    fig.colorbar(spec_im, ax=axs[0], shrink=0.9, pad=0.01)\n",
    "\n",
    "    # Plot the mask\n",
    "    mask_im = axs[1].imshow(mask, aspect=\"auto\", interpolation=\"none\", cmap=\"jet\")\n",
    "    axs[1].set_yticks(range(len(mask_labels)), labels=mask_labels)\n",
    "    fig.colorbar(mask_im, ax=axs[1], shrink=0.9, pad=0.01)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def labels_to_mask(\n",
    "    audio_dir: str,\n",
    "    audio_name: str,\n",
    "    audio_annot: pd.DataFrame,\n",
    "    frame_size: int,\n",
    "    hop_size: int,\n",
    "    n_labels: int,\n",
    "    labels2idx: Dict[str, int],\n",
    "    silence_label: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Given a DataFrame with all the annotations of an audio file, creates a binary\n",
    "    2D tensor with shape (n_labels, n_frames) containing the labels at each frame\n",
    "    (for a given `frame_size` and `hop_size`)\n",
    "\n",
    "    Args:\n",
    "        audio_dir (str): Path to the folder with the audio data\n",
    "        audio_name (str): name of the audio file (\"path\" column in raw data)\n",
    "        audio_annot (pd.DataFrame): Annotations of the audio file to mask\n",
    "        frame_size (int): Frame size (is samples) to create the mask\n",
    "        hop_size (int): Hop size (is samples) to create the mask\n",
    "        n_labels (int): Number of labels to use in the mask\n",
    "        labels2idx (Dict[str, int]): Mapping from label name to index in the mask\n",
    "        silence_label (bool): To add an additional label (with idx 0) for silence\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Binary mask of the labels\n",
    "    \"\"\"\n",
    "    # Load the audio\n",
    "    signal, sample_rate = torchaudio.load(os.path.join(audio_dir, f\"{audio_name}.wav\"))\n",
    "\n",
    "    # Prepare the mask 2D matrix (labels, frames)\n",
    "    n_frames = int((signal.shape[-1] - frame_size) / hop_size) + 1\n",
    "    mask = torch.zeros((n_labels, n_frames))\n",
    "    # Compute some utility values\n",
    "    sample_time = 1 / sample_rate\n",
    "    hop_time = sample_time * hop_size\n",
    "    for _, row in audio_annot.iterrows():\n",
    "        start_frame = int(row.start / hop_time)\n",
    "        end_frame = int(row.end / hop_time)\n",
    "        mask[labels2idx[row.label], start_frame:end_frame] += 1\n",
    "\n",
    "    if silence_label:\n",
    "        silence_mask = mask.sum(axis=0) < 1\n",
    "        assert mask[0, :].sum() == 0  # Should be empty\n",
    "        mask[0, :] = silence_mask\n",
    "\n",
    "    return mask.bool().int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTTATIONS_PATH = \"../dataset/labels.csv\"\n",
    "AUDIO_DIR = \"../dataset/audios\"\n",
    "AUDIO_ID = \"559e3da599a31024cf3744e61a788309\"\n",
    "FRAME_SIZE = 2048  # To compute the spectrogram\n",
    "HOP_SIZE = 1024  # To compute the spectrogram\n",
    "CHUNK_SIZE = 128  # Number of spectrogram frames to take for audio chunk\n",
    "SAMPLE_RATE = 50000  # All the dataset is sampled at 50kHz\n",
    "SPEC_TYPE = \"mel\" # \"mel\" or \"base\"\n",
    "N_MELS = 128\n",
    "SILENCE_LABEL = True  # Add the \"silence\" label at idx 0\n",
    "\n",
    "sample_duration = 1 / SAMPLE_RATE\n",
    "frame_duration = sample_duration * FRAME_SIZE\n",
    "hop_duration = sample_duration * HOP_SIZE\n",
    "chunk_duration = ((CHUNK_SIZE - 1) * hop_duration) + frame_duration\n",
    "print(f\"{sample_duration=:.6f}s\")\n",
    "print(f\"{frame_duration=:.3f}s\")\n",
    "print(f\"{hop_duration=:.3f}s\")\n",
    "print(f\"{chunk_duration=:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(ANNOTTATIONS_PATH)\n",
    "annotations.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels\n",
    "sorted_labels = list(annotations.label.value_counts().index)\n",
    "if SILENCE_LABEL:\n",
    "    sorted_labels = [\"silence\"] + sorted_labels\n",
    "n_labels = len(sorted_labels)\n",
    "labels2idx = {l: i for i, l in enumerate(sorted_labels)}\n",
    "labels2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the annotations corresponding to the selected audio\n",
    "audio_annot = annotations[annotations.path == AUDIO_ID]\n",
    "audio_annot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio signal\n",
    "audio_path = audio_annot.path.values[0]\n",
    "signal, _ = torchaudio.load(os.path.join(AUDIO_DIR, f\"{audio_path}.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the transformation to create the spectrogram\n",
    "if SPEC_TYPE == \"base\":\n",
    "    spec_transform = torchaudio.transforms.Spectrogram(\n",
    "        n_fft=FRAME_SIZE,\n",
    "        hop_length=HOP_SIZE,\n",
    "        center=False\n",
    "    )\n",
    "elif SPEC_TYPE == \"mel\":\n",
    "    spec_transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE, \n",
    "        n_fft=FRAME_SIZE, \n",
    "        win_length=FRAME_SIZE,\n",
    "        hop_length=HOP_SIZE, \n",
    "        center=False, \n",
    "        n_mels=N_MELS\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected spectrogram type ('{SPEC_TYPE}')\")\n",
    "\n",
    "# Compute the spectrogram\n",
    "spectrogram = spec_transform(signal)\n",
    "# Compute the binary mask\n",
    "mask = labels_to_mask(\n",
    "    AUDIO_DIR,\n",
    "    audio_path,\n",
    "    audio_annot,\n",
    "    FRAME_SIZE,\n",
    "    HOP_SIZE,\n",
    "    n_labels,\n",
    "    labels2idx,\n",
    "    silence_label=True,\n",
    ")\n",
    "print(f\"{spectrogram.shape=}\")\n",
    "print(f\"{mask.shape=}\")\n",
    "spec_mask_plot(spectrogram[0], mask, sorted_labels, figsize=(40, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the whole audio spectrogram and mask into chunks\n",
    "spec_chunks = librosa.util.frame(spectrogram, frame_length=CHUNK_SIZE, hop_length=CHUNK_SIZE)\n",
    "mask_chunks = librosa.util.frame(mask, frame_length=CHUNK_SIZE, hop_length=CHUNK_SIZE)\n",
    "print(f\"{spec_chunks.shape=}\")  # shape: (channels, freq, chunk_frames, chunks)\n",
    "print(f\"{mask_chunks.shape=}\")  # shape: (labels, chunk_frames, chunks)\n",
    "assert spec_chunks.shape[-2:] == mask_chunks.shape[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_TIME_STEP = 15  # Second to select the chunk from the audio\n",
    "chunk_idx = int(SHOW_TIME_STEP / chunk_duration)\n",
    "# Select the target chunk\n",
    "spec_chunk = spec_chunks[:, :, :, chunk_idx].copy()\n",
    "mask_chunk = mask_chunks[:, :, chunk_idx].copy()\n",
    "print(f\"{spec_chunk.shape=}\")\n",
    "print(f\"{mask_chunk.shape=}\")\n",
    "spec_mask_plot(torch.from_numpy(spec_chunk[0]), torch.from_numpy(mask_chunk), sorted_labels, figsize=(20, 15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('audio')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2252593e89e7d3aa9f5262b6f0663e084f31daf75977bd0753d2b19f4a4ff6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
